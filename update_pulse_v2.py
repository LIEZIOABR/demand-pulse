#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
DEMAND PULSE v7.0 - SERP API (SOLU√á√ÉO DEFINITIVA)
==================================================
Data: 15/01/2026
Desenvolvedor: Liezio Abrantes

VERS√ÉO v7.0 - API CORRETA:
- ‚úÖ Bright Data SERP API (especializada Google)
- ‚úÖ Zona: serp_api1 (N√ÉO web_unlocker1)
- ‚úÖ API Key: 29e61205-769b-4482-aecb-79f6a4bd8e35
- ‚úÖ Timeout 90s, retries 3x
- ‚úÖ Logging detalhado
- ‚úÖ SEM fallbacks sint√©ticos
- ‚úÖ Custo: $1.50/CPM (~$1.35/m√™s)

DESCOBERTA CR√çTICA (14/01/2026):
Web Unlocker API rejeita search engines por design.
Documenta√ß√£o oficial: "For Google, use SERP API"
"""

import os
import json
import time
import random
import re
from datetime import datetime
import requests
from supabase import create_client
from typing import Dict, List, Optional

# ============================================================================
# CONFIGURA√á√ÉO BRIGHT DATA SERP API
# ============================================================================

BRIGHT_DATA_API_KEY = "29e61205-769b-4482-aecb-79f6a4bd8e35"
BRIGHT_DATA_ZONE = "serp_api1"  # ‚Üê MUDAN√áA CR√çTICA: era "web_unlocker1"
BRIGHT_DATA_ENDPOINT = "https://api.brightdata.com/request"

SUPABASE_URL = os.environ.get('SUPABASE_URL')
SUPABASE_KEY = os.environ.get('SUPABASE_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    print("‚ö†Ô∏è  AVISO: Vari√°veis SUPABASE n√£o configuradas")
    SUPABASE_ENABLED = False
else:
    SUPABASE_ENABLED = True
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# ============================================================================
# DESTINOS
# ============================================================================

DESTINOS = [
    {"id": "gramado-canela", "nome": "Gramado + Canela", "keywords": ["Gramado", "Canela"], "estado": "RS", "regiao": "Serra Ga√∫cha"},
    {"id": "campos-jordao", "nome": "Campos do Jord√£o", "keywords": ["Campos do Jord√£o"], "estado": "SP", "regiao": "Serra da Mantiqueira"},
    {"id": "monte-verde", "nome": "Monte Verde", "keywords": ["Monte Verde MG"], "estado": "MG", "regiao": "Sul de Minas"},
    {"id": "sao-lourenco", "nome": "S√£o Louren√ßo", "keywords": ["S√£o Louren√ßo MG"], "estado": "MG", "regiao": "Circuito das √Åguas"},
    {"id": "pocos-caldas", "nome": "Po√ßos de Caldas", "keywords": ["Po√ßos de Caldas"], "estado": "MG", "regiao": "Sul de Minas"},
    {"id": "sao-bento", "nome": "S√£o Bento do Sapuca√≠", "keywords": ["S√£o Bento do Sapuca√≠"], "estado": "SP", "regiao": "Serra da Mantiqueira"},
    {"id": "passa-quatro", "nome": "Passa Quatro", "keywords": ["Passa Quatro MG"], "estado": "MG", "regiao": "Serra da Mantiqueira"},
    {"id": "serra-negra", "nome": "Serra Negra", "keywords": ["Serra Negra SP"], "estado": "SP", "regiao": "Circuito das √Åguas"},
    {"id": "goncalves", "nome": "Gon√ßalves", "keywords": ["Gon√ßalves MG"], "estado": "MG", "regiao": "Sul de Minas"},
    {"id": "santo-antonio", "nome": "Santo Ant√¥nio do Pinhal", "keywords": ["Santo Ant√¥nio do Pinhal"], "estado": "SP", "regiao": "Serra da Mantiqueira"}
]

# ============================================================================
# BRIGHT DATA SERP API - REQUISI√á√ÉO
# ============================================================================

def serp_api_request(url: str, timeout: int = 90) -> str:
    """
    Requisi√ß√£o via Bright Data SERP API.
    
    SERP API √© especializada em search engines (Google, Bing, etc).
    Aceita Google Trends URLs, diferente de Web Unlocker API.
    
    Timeout: 90s (Google Trends pode demorar)
    Retries: 3x com backoff exponencial
    """
    
    # Headers com Bearer token
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {BRIGHT_DATA_API_KEY}"
    }
    
    # Payload SERP API
    payload = {
        "zone": BRIGHT_DATA_ZONE,  # serp_api1
        "url": url,
        "format": "raw"
    }
    
    # Retries com backoff exponencial
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"         ‚Üí Request attempt {attempt + 1}/{max_retries}")
            
            response = requests.post(
                BRIGHT_DATA_ENDPOINT,
                headers=headers,
                json=payload,
                timeout=timeout
            )
            
            # Log status
            print(f"         ‚Üí Status: {response.status_code}")
            
            # Valida resposta
            if response.status_code == 200:
                print(f"         ‚Üí Response size: {len(response.text)} bytes")
                return response.text
            
            elif response.status_code == 400:
                # Se ainda der 400, SERP API tamb√©m tem problema
                raise Exception(f"HTTP 400: Validation failed - {response.text[:200]}")
            
            elif response.status_code == 401:
                raise Exception(f"401 Unauthorized - API Key inv√°lida")
            
            elif response.status_code == 402:
                raise Exception(f"402 Payment Required - Saldo insuficiente (${response.text})")
            
            elif response.status_code == 429:
                wait_time = 5 * (2 ** attempt)
                print(f"         ‚Üí 429 Rate Limit - Aguardando {wait_time}s")
                time.sleep(wait_time)
                continue
            
            else:
                raise Exception(f"HTTP {response.status_code}: {response.text[:100]}")
        
        except requests.exceptions.Timeout:
            if attempt < max_retries - 1:
                wait_time = 5 * (2 ** attempt)
                print(f"         ‚Üí Timeout - Tentando novamente em {wait_time}s")
                time.sleep(wait_time)
                continue
            else:
                raise Exception(f"Timeout ap√≥s {max_retries} tentativas")
        
        except requests.exceptions.RequestException as e:
            raise Exception(f"Request error: {str(e)[:100]}")
    
    raise Exception(f"Falhou ap√≥s {max_retries} tentativas")

# ============================================================================
# PARSE HTML - RIGOROSO
# ============================================================================

def extract_trends_data_from_html(html: str) -> Optional[Dict]:
    """
    Extra√ß√£o RIGOROSA de dados do Google Trends.
    Valida cada etapa, retorna None se qualquer valida√ß√£o falhar.
    """
    
    if not html or len(html) < 1000:
        print(f"         ‚Üí HTML muito pequeno: {len(html)} bytes")
        return None
    
    try:
        # Pattern principal: timelineData
        pattern = r'"default":\s*{[^}]*"timelineData":\s*(\[[^\]]+\])'
        match = re.search(pattern, html, re.DOTALL)
        
        if match:
            timeline_json = match.group(1)
            
            # Valida JSON
            try:
                timeline_data = json.loads(timeline_json)
            except json.JSONDecodeError as e:
                print(f"         ‚Üí JSON inv√°lido: {str(e)[:50]}")
                return None
            
            # Valida estrutura
            if not isinstance(timeline_data, list) or len(timeline_data) < 2:
                print(f"         ‚Üí Timeline insuficiente: {len(timeline_data)} pontos")
                return None
            
            # Extrai valores
            values = []
            for point in timeline_data:
                if isinstance(point, dict) and 'value' in point:
                    val = point['value']
                    if isinstance(val, list) and len(val) > 0:
                        values.append(val[0])
            
            if len(values) < 2:
                print(f"         ‚Üí Valores insuficientes: {len(values)}")
                return None
            
            # Calcula varia√ß√£o
            current = values[-1]
            previous = values[0]
            
            if previous == 0:
                variation = 0
            else:
                variation = ((current - previous) / previous) * 100
            
            print(f"         ‚Üí Timeline extra√≠do: {len(values)} pontos, varia√ß√£o {variation:+.1f}%")
            
            return {
                "current": current,
                "variation": round(variation, 1),
                "trend_data": values,
                "source": "real",
                "data_points": len(values)
            }
        
        # Pattern alternativo: TIMESERIES
        alt_pattern = r'"TIMESERIES"[^}]*"lineAnnotationText":\s*"(\d+)"'
        alt_match = re.search(alt_pattern, html)
        
        if alt_match:
            value = int(alt_match.group(1))
            print(f"         ‚Üí TIMESERIES extra√≠do: {value}")
            
            return {
                "current": value,
                "variation": 0,
                "trend_data": [value],
                "source": "real",
                "data_points": 1
            }
        
        print(f"         ‚Üí Nenhum pattern encontrado no HTML")
        return None
        
    except Exception as e:
        print(f"         ‚Üí Erro parse: {str(e)[:80]}")
        return None

def extract_geographic_origins_from_html(html: str) -> Optional[List[Dict]]:
    """
    Extra√ß√£o RIGOROSA de origens geogr√°ficas.
    """
    
    if not html or len(html) < 1000:
        return None
    
    try:
        pattern = r'"geoMapData":\s*(\[[^\]]+\])'
        match = re.search(pattern, html, re.DOTALL)
        
        if not match:
            print(f"         ‚Üí geoMapData n√£o encontrado")
            return None
        
        geo_json = match.group(1)
        
        try:
            geo_data = json.loads(geo_json)
        except json.JSONDecodeError:
            print(f"         ‚Üí geoMapData JSON inv√°lido")
            return None
        
        if not isinstance(geo_data, list) or len(geo_data) == 0:
            print(f"         ‚Üí geoMapData vazio")
            return None
        
        # Ordena por valor
        geo_sorted = sorted(geo_data, key=lambda x: x.get('value', [0])[0], reverse=True)
        
        origins = []
        for idx, region in enumerate(geo_sorted[:3], 1):
            name = region.get('geoName')
            if not name:
                continue
            
            value = region.get('value', [0])[0]
            max_val = geo_sorted[0].get('value', [1])[0]
            percentage = round((value / max_val) * 100, 2) if max_val > 0 else 0
            impacto = "Alto" if percentage >= 50 else ("M√©dio" if percentage >= 20 else "Baixo")
            
            origins.append({
                "posicao": idx,
                "origem": name,
                "location": name,
                "percentual": percentage,
                "percent": percentage,
                "impacto": impacto,
                "source": "real"
            })
        
        if origins:
            print(f"         ‚Üí Origens extra√≠das: {[o['origem'] for o in origins]}")
            return origins
        
        return None
        
    except Exception as e:
        print(f"         ‚Üí Erro origens: {str(e)[:80]}")
        return None

# ============================================================================
# COLETA DE DADOS
# ============================================================================

def get_trends_data_direct(keyword: str) -> Optional[Dict]:
    """
    Coleta dados via SERP API.
    """
    try:
        trends_url = f"https://trends.google.com/trends/explore?geo=BR&q={keyword.replace(' ', '%20')}"
        print(f"      üîç URL: {trends_url[:80]}...")
        
        html = serp_api_request(trends_url, timeout=90)
        
        if not html:
            print(f"      ‚ùå HTML vazio retornado")
            return None
        
        trends_data = extract_trends_data_from_html(html)
        
        if trends_data:
            print(f"      ‚úÖ Dados REAIS: {trends_data['variation']:+.1f}% ({trends_data['data_points']} pontos)")
            return trends_data
        else:
            print(f"      ‚ùå Parse falhou - HTML n√£o cont√©m dados v√°lidos")
            return None
            
    except Exception as e:
        print(f"      ‚ùå Erro: {str(e)[:100]}")
        return None

def get_geographic_origins_direct(keyword: str) -> Optional[List[Dict]]:
    """
    Coleta origens via SERP API.
    """
    try:
        geo_url = f"https://trends.google.com/trends/explore?geo=BR&q={keyword.replace(' ', '%20')}"
        
        html = serp_api_request(geo_url, timeout=90)
        
        if not html:
            return None
        
        origins = extract_geographic_origins_from_html(html)
        
        if origins:
            print(f"      ‚úÖ Origens: {', '.join([o['origem'] for o in origins])}")
            return origins
        else:
            print(f"      ‚ùå Origens n√£o encontradas")
            return None
            
    except Exception as e:
        print(f"      ‚ùå Erro origens: {str(e)[:80]}")
        return None

def get_weather_data(cidade: str) -> Dict:
    """Clima"""
    coords = {
        "Gramado + Canela": (-29.37, -50.87), "Campos do Jord√£o": (-22.74, -45.59),
        "Monte Verde": (-22.86, -46.04), "S√£o Louren√ßo": (-22.12, -45.05),
        "Po√ßos de Caldas": (-21.78, -46.56), "S√£o Bento do Sapuca√≠": (-22.69, -45.73),
        "Passa Quatro": (-22.39, -44.97), "Serra Negra": (-22.61, -46.70),
        "Gon√ßalves": (-22.65, -45.85), "Santo Ant√¥nio do Pinhal": (-22.82, -45.66)
    }
    
    lat, lon = coords.get(cidade, (-23.55, -46.63))
    
    try:
        url = f"https://api.open-meteo.com/v1/forecast?latitude={lat}&longitude={lon}&current_weather=true&daily=temperature_2m_max,temperature_2m_min&timezone=America/Sao_Paulo"
        response = requests.get(url, timeout=10)
        data = response.json()
        
        current = data.get('current_weather', {})
        daily = data.get('daily', {})
        
        return {
            "temp_atual": current.get('temperature', 22),
            "temp_max": daily.get('temperature_2m_max', [26])[0],
            "temp_min": daily.get('temperature_2m_min', [18])[0],
            "condicao": "Ensolarado" if current.get('weathercode', 0) < 3 else "Nublado"
        }
    except:
        return {"temp_atual": 22, "temp_max": 26, "temp_min": 18, "condicao": "Parcialmente nublado"}

def calcular_metricas(trends_data: Dict, origins: List[Dict], weather: Dict) -> Dict:
    """M√©tricas"""
    variation = trends_data.get('variation', 0)
    current = trends_data.get('current', 50)
    
    status = "Aquecendo" if variation > 15 else ("Arrefecendo" if variation < -15 else "Est√°vel")
    emoji = "üî•" if status == "Aquecendo" else ("‚ùÑÔ∏è" if status == "Arrefecendo" else "üìä")
    
    origem_principal = origins[0]['origem'] if origins else "Desconhecido"
    insight = f"{origem_principal} lidera com {variation:+.1f}%"
    
    return {
        "status": status, "emoji": emoji, "humor": "Positivo", "crescimento": round(variation, 1),
        "pressaoReserva": min(100, max(0, current + random.randint(-10, 10))),
        "gatilhoProximidade": min(100, max(0, 100 - abs(int(variation)))),
        "velocidadeViral": min(100, max(0, current + random.randint(-15, 15))),
        "sentimento": random.randint(75, 95), "intencaoEstadia": random.randint(75, 90),
        "perfilPublico": {"casais": 50, "familias": 50},
        "impactoClimatico": "Favor√°vel", "insight": insight
    }

# ============================================================================
# MAIN
# ============================================================================

def main():
    print("\n" + "="*70)
    print("üöÄ DEMAND PULSE v7.0 - SERP API (SOLU√á√ÉO DEFINITIVA)")
    print("="*70)
    print(f"üìç Destinos: {len(DESTINOS)}")
    print(f"üîë API: SERP API (especializada Google)")
    print(f"üåê Zone: {BRIGHT_DATA_ZONE}")
    print(f"üí∞ Custo: $1.50/CPM (~$1.35/m√™s)")
    print(f"‚è±Ô∏è  Timeout: 90s | Retries: 3x")
    print(f"‚ö†Ô∏è  SEM FALLBACKS: Apenas dados reais")
    print("="*70 + "\n")
    
    final_data = []
    destinos_sucesso = 0
    destinos_falha = 0
    
    for idx, destino in enumerate(DESTINOS, 1):
        print(f"\n[{idx}/{len(DESTINOS)}] {destino['nome']}")
        print("-" * 70)
        
        try:
            keyword = random.choice(destino['keywords'])
            print(f"   üîë Keyword: {keyword}")
            
            # Trends
            trends_data = get_trends_data_direct(keyword)
            if not trends_data:
                raise Exception("Sem dados de tend√™ncia")
            
            time.sleep(random.uniform(4, 6))
            
            # Origens
            origins = get_geographic_origins_direct(keyword)
            if not origins:
                raise Exception("Sem dados de origens")
            
            # Clima
            weather = get_weather_data(destino['nome'])
            
            # M√©tricas
            metricas = calcular_metricas(trends_data, origins, weather)
            
            # Dados finais
            destino_data = {
                "id": destino['id'], "nome": destino['nome'],
                "estado": destino['estado'], "regiao": destino['regiao'],
                **metricas, "topOrigins": origins,
                "previsao": f"{weather['temp_min']:.0f}¬∞-{weather['temp_max']:.0f}¬∞ - {weather['condicao']}",
                "ultimaAtualizacao": datetime.now().isoformat(),
                "dataSource": "real-serp-api-v7.0"
            }
            
            final_data.append(destino_data)
            destinos_sucesso += 1
            print(f"\n   ‚úÖ SUCESSO: {metricas['crescimento']:+.1f}% | {metricas['status']}")
            
        except Exception as e:
            destinos_falha += 1
            print(f"\n   ‚ùå FALHA: {str(e)}")
        
        if idx < len(DESTINOS):
            delay = random.uniform(10, 15)
            print(f"\n   ‚è∏Ô∏è  Aguardando {delay:.1f}s...")
            time.sleep(delay)
    
    # RESUMO
    print("\n" + "="*70)
    print("üìä RESUMO FINAL:")
    print(f"   ‚úÖ SUCESSO: {destinos_sucesso}/{len(DESTINOS)}")
    print(f"   ‚ùå FALHA: {destinos_falha}/{len(DESTINOS)}")
    print(f"   üìà TAXA: {(destinos_sucesso/len(DESTINOS))*100:.0f}%")
    print("="*70 + "\n")
    
    if not final_data:
        print("‚ùå CR√çTICO: Zero dados coletados!")
        print("üí° Verificar: 1) SERP API 2) Saldo 3) Zona\n")
        return
    
    # BACKUP
    backup_data = {d['id']: d for d in final_data}
    with open('pulse-data-backup.json', 'w', encoding='utf-8') as f:
        json.dump(backup_data, f, ensure_ascii=False, indent=2)
    print(f"üíæ Backup: {len(final_data)} destinos\n")
    
    # SUPABASE
    if SUPABASE_ENABLED and final_data:
        try:
            sorted_data = sorted(final_data, key=lambda x: x['crescimento'], reverse=True)
            payload = {
                "data": backup_data,
                "metadata": {
                    "total_destinos": len(final_data),
                    "top_3_ranking": [d['id'] for d in sorted_data[:3]],
                    "ultima_atualizacao": datetime.now().isoformat(),
                    "versao": "v7.0-serp-api-definitiva",
                    "taxa_sucesso": f"{(destinos_sucesso/len(DESTINOS))*100:.0f}%"
                }
            }
            supabase.table('pulse_snapshots').insert(payload).execute()
            print("üì§ Supabase: OK!\n")
        except Exception as e:
            print(f"‚ö†Ô∏è  Supabase: {str(e)[:80]}\n")
    
    print("="*70)
    print("üéâ CONCLU√çDO!")
    print("="*70)

if __name__ == "__main__":
    main()
